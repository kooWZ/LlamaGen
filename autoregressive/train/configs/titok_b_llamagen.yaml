dataset: "imagenet_code"
num_classes: 1000
code_path: "ImageNet-1k/titok_codes/h5_dataset/"

auto_resume: true
# gpt_ckpt: "outputs/ckpts/92_1163523.pt"
results_dir: "outputs/titok_B_llamagen_recipe"
upload_to_hf: false

ema: false

gpt_model: "GPT-B"
gpt_type: "c2i"
vocab_size: 8192
cls_token_num: 1
dropout_p: 0.1
token_dropout_p: 0.1
drop_path_rate: 0.0
latent_size: 128
use_liger: false

no_compile: false

epochs: 300
global_batch_size: 256
global_seed: 42
num_workers: 16
gradient_accumulation_steps: 1
mixed_precision: "bf16"
fp32_attention: true

max_grad_norm: 1
grad_norm_threshold: 10.0
check_grad_norm_for_skip: false

ckpt_every_epoch: 10
do_eval: true
# eval_every_iter: 20
reduce_grad_norm_every_iter: 1

lr: 1.0e-4
init_lr: 1.0e-6
final_lr: 1.2e-5
weight_decay: 5.0e-2
scheduler: "constant"
beta1: 0.9
beta2: 0.95
eps: 1.0e-6
warmup_epochs: 80

wandb_project: "LlamaGen"
wandb_run_name: "titok_B_llamagen_recipe"
wandb_tags: []
wandb_notes: null
wandb_entity: "koowz-FVL25"
wandb_key: null

decoder_type: "titok"
vq_ckpt: "outputs/ckpts/titok_bl128/"
eval_per_gpu_batch_size: 64
eval_num_fid_samples: 50000
eval_gather_freq: 10
cfg_scale: 2
cfg_interval: -1
temperature: 1.0
top_k: 0
top_p: 1.0
decoder_timesteps: 25
decoder_guidance_scale: 15